\section{Conclusions}
As we expected, for common files and also for a random sequence of bytes (\texttt{ran}), the commercial software behaves better than our own implementation, thanks to the different steps of compression that have been applied. Another aspect we could have guessed is that the pure LZ77 algorithm is the worst one. Anyhow, one could wonder why in the first case, \texttt{rep.txt}, our versions are the best ones. This is due to the additive information brought by the LZ77 Deflate algorithm: the Huffman coding is very useful in a generic case, but the overall information needed to decode it could actually be over-necessary if the file is very redundant. On the other hand, we can see that, for a high entropy file as \texttt{ran} the commercial software is very robust, even though no ones among the algorithms can go below the $100$\% compression threshold, which is, actually, impossible from information theory. In this case, however, additional Huffman and run-length coding manage to approach the maximum possible compression. In all the remaining cases, \texttt{zip} and \texttt{gzip} act better than our programs, but we can note that the general trend is almost the same for all the four programs.
